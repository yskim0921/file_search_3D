# search_visualize_app.py (ìš”ì•½ ë‚´ìš© ì „ì²´ í‘œì‹œ ë²„ì „)

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from typing import TypedDict, List
from langchain import PromptTemplate
from langchain_community.llms import Ollama
from langchain.chains import LLMChain
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langgraph.graph import StateGraph, END
import pymysql

# ê¸°ì¡´ ì½”ë“œì˜ ì„¤ì •ë“¤
class AgentState(TypedDict):
    query: str
    keywords: str
    file_paths: list
    relevant_docs: str
    result: str
    search_vectors: list

DB_CONFIG = {
    'host': 'localhost',
    'user': 'admin',
    'password': '1qazZAQ!',
    'db': 'final',
    'charset': 'utf8mb4'
}

# SVD ê¸°ë°˜ PCA (ì´ì „ê³¼ ë™ì¼)
def robust_pca(X, n_components=3):
    try:
        X = np.array(X, dtype=np.float64)
        
        if X.shape[0] < 2:
            st.warning("ë°ì´í„°ê°€ ë„ˆë¬´ ì ì–´ PCAë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return X
        
        X_mean = np.mean(X, axis=0)
        X_centered = X - X_mean
        U, s, Vt = np.linalg.svd(X_centered, full_matrices=False)
        n_components = min(n_components, X_centered.shape[1], X_centered.shape[0])
        V = Vt[:n_components].T
        X_reduced = X_centered @ V
        X_reduced = np.real(X_reduced)
        
        return X_reduced
        
    except Exception as e:
        st.error(f"PCA ê³„ì‚° ì˜¤ë¥˜: {str(e)}")
        if X.shape[1] >= 2:
            return X[:, :min(3, X.shape[1])]
        else:
            return np.column_stack([X.flatten(), np.zeros(X.shape[0]), np.zeros(X.shape[0])])

# Streamlit ì„¤ì •
st.set_page_config(page_title="ğŸ” 3D ê²€ìƒ‰ ì‹œê°í™”", layout="wide")
st.title("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ & 3D ì‹œê°í™”")
st.markdown("ê²€ìƒ‰ ê²°ê³¼ë¥¼ 3D ê³µê°„ì—ì„œ ì‹œê°í™”í•˜ì—¬ ë¬¸ì„œ ê°„ì˜ ê´€ê³„ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤.")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'search_results' not in st.session_state:
    st.session_state.search_results = None
if 'vectors_data' not in st.session_state:
    st.session_state.vectors_data = None

@st.cache_resource
def get_llm():
    return Ollama(model="exaone3.5:2.4b")

@st.cache_resource
def get_embeddings():
    return OllamaEmbeddings(model="exaone3.5:2.4b")

llm = get_llm()
embeddings = get_embeddings()

def rag_agent_with_vectors(state: AgentState):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í•˜ê³  ë²¡í„° ì •ë³´ë„ í•¨ê»˜ ë°˜í™˜"""
    try:
        vectorstore = Chroma(
            persist_directory="./rag_chroma/documents/summary/", 
            embedding_function=embeddings
        )
        
        results = vectorstore.similarity_search_with_score(state["keywords"], k=10)
        
        if not results:
            return {
                **state,
                "file_paths": [],
                "relevant_docs": "",
                "search_vectors": []
            }
        
        search_vectors = []
        file_entries = []
        
        scores = [score for _, score in results]
        min_score = min(scores)
        max_score = max(scores)
        
        query_vector = embeddings.embed_query(state["keywords"])
        
        if not isinstance(query_vector, list) or len(query_vector) == 0:
            st.error("ì¿¼ë¦¬ ë²¡í„° ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return {**state, "file_paths": [], "relevant_docs": "", "search_vectors": []}
        
        # ğŸ”§ ìˆ˜ì •: ë¬¸ì„œ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì œê±°
        for doc, score in results:
            doc_text = doc.page_content
            
            if not doc_text or len(doc_text.strip()) == 0:
                continue
                
            try:
                # ğŸ”§ ìˆ˜ì •: í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œì„ ëŠ˜ë¦¼ (500 â†’ 2000)
                doc_vector = embeddings.embed_query(doc_text[:2000])
                
                if not isinstance(doc_vector, list) or len(doc_vector) != len(query_vector):
                    continue
                
                if max_score == min_score:
                    similarity = 100.0
                else:
                    similarity = (1 - (score - min_score)/(max_score - min_score)) * 100
                
                search_vectors.append({
                    'vector': doc_vector,
                    'text': doc_text,  # ğŸ”§ ìˆ˜ì •: ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥ (ìë¥´ì§€ ì•ŠìŒ)
                    'similarity': similarity,
                    'doc_id': doc.metadata.get('id'),
                    'is_query': False
                })
            except Exception as e:
                st.warning(f"ë¬¸ì„œ ë²¡í„° ìƒì„± ì‹¤íŒ¨: {str(e)}")
                continue
        
        search_vectors.append({
            'vector': query_vector,
            'text': f"ê²€ìƒ‰ ì¿¼ë¦¬: {state['keywords']}",
            'similarity': 100.0,
            'doc_id': 'QUERY',
            'is_query': True
        })
        
        # MySQLì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ
        conn = None
        try:
            conn = pymysql.connect(**DB_CONFIG)
            cursor = conn.cursor()
            
            for item in search_vectors[:-1]:
                if item['doc_id']:
                    cursor.execute("""
                        SELECT file_name, file_location, summary 
                        FROM documents WHERE id = %s
                    """, (item['doc_id'],))
                    row = cursor.fetchone()
                    if row:
                        item['file_name'] = row[0]
                        item['file_location'] = row[1]
                        # ğŸ”§ ìˆ˜ì •: ì „ì²´ ìš”ì•½ ì €ì¥ (ìë¥´ì§€ ì•ŠìŒ)
                        item['full_summary'] = row[2]
                        file_entries.append({
                            'name': row[0],
                            'location': row[1],
                            'summary': row[2],  # ğŸ”§ ìˆ˜ì •: ì „ì²´ ìš”ì•½ ì €ì¥
                            'relevance': round(item['similarity'], 1)
                        })
        except Exception as e:
            st.warning(f"DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
        finally:
            if conn:
                conn.close()
        
        doc_summary = "\n".join([d.page_content for d, _ in results])
        
        return {
            **state, 
            "file_paths": file_entries, 
            "relevant_docs": doc_summary,
            "search_vectors": search_vectors
        }
        
    except Exception as e:
        st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {**state, "file_paths": [], "relevant_docs": "", "search_vectors": []}

# ë©”ì¸ UI
col1, col2 = st.columns([1, 2])

with col1:
    st.subheader("ğŸ” ê²€ìƒ‰")
    query = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", placeholder="ì˜ˆ: ì¹´ì¹´ì˜¤í†¡")
    
    if st.button("ê²€ìƒ‰ ì‹¤í–‰", type="primary"):
        if query:
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                try:
                    keyword_prompt = PromptTemplate.from_template(
                        """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì—ì„œ ë²¡í„°ìŠ¤í† ì–´ ê²€ìƒ‰ì„ ìœ„í•œ ìµœì ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
                        ì§ˆë¬¸: {query}"""
                    )
                    chain = LLMChain(llm=llm, prompt=keyword_prompt)
                    keywords = chain.run({"query": query}).strip()
                    
                    st.info(f"ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
                    
                    state = {
                        "query": query,
                        "keywords": keywords,
                        "file_paths": [],
                        "relevant_docs": "",
                        "result": "",
                        "search_vectors": []
                    }
                    
                    result = rag_agent_with_vectors(state)
                    st.session_state.search_results = result['file_paths']
                    st.session_state.vectors_data = result['search_vectors']
                    
                    if not st.session_state.search_results:
                        st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")
                    
                except Exception as e:
                    st.error(f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
                    
        else:
            st.error("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    
    # ğŸ”§ ìˆ˜ì •: ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ (ìš”ì•½ ë‚´ìš© ì „ì²´ í‘œì‹œ)
    if st.session_state.search_results:
        st.subheader("ğŸ“„ ê²€ìƒ‰ ê²°ê³¼")
        
        # ì „ì²´ ìš”ì•½ ë³´ê¸° ì˜µì…˜ ì¶”ê°€
        show_full_summary = st.checkbox("ğŸ“– ì „ì²´ ìš”ì•½ ë³´ê¸°", value=True)
        
        for i, entry in enumerate(st.session_state.search_results[:5], 1):
            with st.expander(f"{i}. {entry['name']} ({entry['relevance']}%)", expanded=False):
                st.markdown(f"**ğŸ“ íŒŒì¼ ìœ„ì¹˜:** `{entry['location']}`")
                st.markdown(f"**ğŸ¯ ê´€ë ¨ì„±:** {entry['relevance']}%")
                
                # ğŸ”§ ìˆ˜ì •: ìš”ì•½ ë‚´ìš© ì „ì²´ í‘œì‹œ
                st.markdown("**ğŸ“ ìš”ì•½ ë‚´ìš©:**")
                if show_full_summary:
                    # ì „ì²´ ìš”ì•½ì„ ìŠ¤í¬ë¡¤ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ ì˜ì—­ì— í‘œì‹œ
                    st.text_area(
                        label="",
                        value=entry['summary'],
                        height=200,
                        key=f"summary_{i}",
                        label_visibility="collapsed"
                    )
                else:
                    # ê°„ë‹¨ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 300ì)
                    preview = entry['summary'][:300]
                    if len(entry['summary']) > 300:
                        preview += "..."
                    st.info(preview)
                    
                    # ì „ì²´ ë³´ê¸° ë²„íŠ¼
                    if st.button(f"ì „ì²´ ìš”ì•½ ë³´ê¸°", key=f"show_full_{i}"):
                        st.text_area(
                            label="ì „ì²´ ìš”ì•½",
                            value=entry['summary'],
                            height=300,
                            key=f"full_summary_{i}"
                        )

with col2:
    st.subheader("ğŸŒŒ 3D ì‹œê°í™”")
    
    if st.session_state.vectors_data and len(st.session_state.vectors_data) > 1:
        try:
            valid_vectors = []
            valid_items = []
            
            for item in st.session_state.vectors_data:
                if 'vector' in item and isinstance(item['vector'], list) and len(item['vector']) > 0:
                    try:
                        vector_array = np.array(item['vector'], dtype=np.float64)
                        if not np.isnan(vector_array).any() and not np.isinf(vector_array).any():
                            valid_vectors.append(vector_array)
                            valid_items.append(item)
                    except:
                        continue
            
            if len(valid_vectors) < 2:
                st.warning("ìœ íš¨í•œ ë²¡í„° ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            else:
                vectors = np.array(valid_vectors)
                
                with st.spinner("3D ë³€í™˜ ì¤‘..."):
                    vectors_3d = robust_pca(vectors, n_components=3)
                
                if vectors_3d is None or vectors_3d.shape[1] < 3:
                    st.error("3D ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                else:
                    colors = []
                    sizes = []
                    texts = []
                    symbols = []
                    
                    for i, item in enumerate(valid_items):
                        if item['is_query']:
                            colors.append('red')
                            sizes.append(20)
                            symbols.append('diamond')
                            texts.append(f"ğŸ” {item['text']}")
                        else:
                            similarity = item['similarity']
                            green_val = max(0, min(255, int(similarity * 2.55)))
                            red_val = max(0, min(255, 255 - green_val))
                            colors.append(f'rgb({red_val}, {green_val}, 100)')
                            sizes.append(max(5, 10 + similarity/10))
                            symbols.append('circle')
                            file_name = item.get('file_name', 'Unknown')
                            # ğŸ”§ ìˆ˜ì •: í˜¸ë²„ í…ìŠ¤íŠ¸ë„ ë” ìì„¸íˆ í‘œì‹œ
                            hover_text = f"{file_name}<br>ìœ ì‚¬ë„: {similarity:.1f}%<br>"
                            # ìš”ì•½ì˜ ì²˜ìŒ 500ìë§Œ í˜¸ë²„ì— í‘œì‹œ (ë„ˆë¬´ ê¸¸ë©´ í™”ë©´ì´ ë³µì¡í•´ì§)
                            summary_preview = item.get('full_summary', item['text'])[:500]
                            if len(item.get('full_summary', item['text'])) > 500:
                                summary_preview += "..."
                            hover_text += summary_preview
                            texts.append(hover_text)
                    
                    # Plotly 3D ì‚°ì ë„
                    fig = go.Figure()
                    
                    for i in range(len(vectors_3d)):
                        fig.add_trace(go.Scatter3d(
                            x=[float(vectors_3d[i, 0])],
                            y=[float(vectors_3d[i, 1])],
                            z=[float(vectors_3d[i, 2])],
                            mode='markers',
                            marker=dict(
                                size=sizes[i],
                                color=colors[i],
                                symbol=symbols[i],
                                line=dict(
                                    width=3 if valid_items[i]['is_query'] else 1,
                                    color='DarkSlateGrey'
                                ),
                                opacity=0.9 if valid_items[i]['is_query'] else 0.7
                            ),
                            text=texts[i],
                            hoverinfo='text',
                            showlegend=False,
                            name=f"Point {i}"
                        ))
                    
                    # ì—°ê²°ì„  ì¶”ê°€
                    query_idx = len(vectors_3d) - 1
                    for i in range(len(vectors_3d) - 1):
                        fig.add_trace(go.Scatter3d(
                            x=[float(vectors_3d[query_idx, 0]), float(vectors_3d[i, 0])],
                            y=[float(vectors_3d[query_idx, 1]), float(vectors_3d[i, 1])],
                            z=[float(vectors_3d[query_idx, 2]), float(vectors_3d[i, 2])],
                            mode='lines',
                            line=dict(
                                color='lightgray',
                                width=2,
                                dash='dot'
                            ),
                            opacity=0.4,
                            showlegend=False,
                            hoverinfo='skip',
                            name=f"Connection {i}"
                        ))
                    
                    fig.update_layout(
                        title="ê²€ìƒ‰ ê²°ê³¼ì˜ 3D ë²¡í„° ê³µê°„",
                        scene=dict(
                            xaxis_title="ì£¼ì œ ìœ ì‚¬ì„± (Topic Similarity)",
                            yaxis_title="ì˜ë¯¸ì  ë³µì¡ë„ (Semantic Complexity)", 
                            zaxis_title="ë¬¸ë§¥ ê´€ë ¨ì„± (Contextual Relevance)",
                            camera=dict(
                                eye=dict(x=1.5, y=1.5, z=1.5)
                            ),
                            aspectmode='cube'
                        ),
                        height=600,
                        showlegend=False,
                        margin=dict(l=0, r=0, t=50, b=0)
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # í†µê³„ ì •ë³´
                    if st.session_state.search_results:
                        col2_1, col2_2, col2_3 = st.columns(3)
                        with col2_1:
                            st.metric("ê²€ìƒ‰ëœ ë¬¸ì„œ", len(st.session_state.search_results))
                        with col2_2:
                            avg_sim = np.mean([item['relevance'] for item in st.session_state.search_results])
                            st.metric("í‰ê·  ìœ ì‚¬ë„", f"{avg_sim:.1f}%")
                        with col2_3:
                            max_sim = max([item['relevance'] for item in st.session_state.search_results])
                            st.metric("ìµœê³  ìœ ì‚¬ë„", f"{max_sim:.1f}%")
                
        except Exception as e:
            st.error(f"ì‹œê°í™” ì˜¤ë¥˜: {str(e)}")
            st.info("ë‹¤ì‹œ ê²€ìƒ‰ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
    else:
        st.info("ê²€ìƒ‰ì„ ì‹¤í–‰í•˜ë©´ ê²°ê³¼ê°€ 3Dë¡œ ì‹œê°í™”ë©ë‹ˆë‹¤.")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì‹œìŠ¤í…œ ìƒíƒœ")
    if st.session_state.vectors_data:
        st.success(f"ë²¡í„° {len(st.session_state.vectors_data)}ê°œ ë¡œë“œë¨")
    else:
        st.info("ê²€ìƒ‰ ëŒ€ê¸° ì¤‘")
    
    st.header("ğŸ“‹ ë„ì›€ë§")
    st.markdown("""
    ### ì‚¬ìš©ë²•
    1. ê²€ìƒ‰ì–´ ì…ë ¥ í›„ 'ê²€ìƒ‰ ì‹¤í–‰' í´ë¦­
    2. ì™¼ìª½ì—ì„œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
    3. ì˜¤ë¥¸ìª½ì—ì„œ 3D ì‹œê°í™” íƒìƒ‰
    
    ### íŒ
    - expanderë¥¼ í´ë¦­í•˜ë©´ ì „ì²´ ìš”ì•½ ë‚´ìš© í™•ì¸ ê°€ëŠ¥
    - 'ì „ì²´ ìš”ì•½ ë³´ê¸°' ì²´í¬ë°•ìŠ¤ë¡œ í‘œì‹œ ë°©ì‹ ë³€ê²½
    - 3D ê·¸ë˜í”„ëŠ” ë§ˆìš°ìŠ¤ë¡œ íšŒì „/í™•ëŒ€ ê°€ëŠ¥
    """)
    
    if st.button("ğŸ”„ ì´ˆê¸°í™”"):
        st.session_state.search_results = None
        st.session_state.vectors_data = None
        st.rerun()