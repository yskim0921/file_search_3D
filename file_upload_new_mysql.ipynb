{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1a3f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/file_upload/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… docx2txt íŒ¨í‚¤ì§€ í™•ì¸ë¨\n",
      "âœ… pypdf íŒ¨í‚¤ì§€ í™•ì¸ë¨\n",
      "ğŸ” ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: fileList/êµ­ë¯¼ì˜êµ°ëŒ€.docx\n",
      "ğŸ“„ ì²˜ë¦¬ ì¤‘: fileList/êµ­ë¯¼ì˜êµ°ëŒ€.docx\n",
      "ğŸ“‹ íŒŒì¼ íƒ€ì…: .docx\n",
      "âœ… êµ­ë¯¼ì˜êµ°ëŒ€.docx ë¡œë“œ ì™„ë£Œ. ì²­í¬ ìˆ˜: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3400570/3988143294.py:87: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"exaone3.5:2.4b\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… êµ­ë¯¼ì˜êµ°ëŒ€.docx DB ì €ì¥ ì™„ë£Œ!\n",
      "   íŒŒì¼íƒ€ì…: .docx\n",
      "   ì œëª©: ì´ì¬ëª… ëŒ€í†µë ¹, êµ­êµ°ì˜ ë‚  ê¸°ë…í–‰ì‚¬ ì£¼ì¬ ë° êµ­ë¯¼êµ°ëŒ€ ...\n",
      "   ìš”ì•½: ì´ì¬ëª… ëŒ€í†µë ¹ì´ ê±´êµ° 77ì£¼ë…„ êµ­êµ°ì˜ ë‚  í–‰ì‚¬ë¥¼ ê³„ë£¡ëŒ€ì—ì„œ ì£¼ì¬í•˜ë©°, êµ­ë¯¼ê³¼ í•¨ê»˜í•˜ëŠ” 'êµ­...\n",
      "=============================================================\n",
      "\n",
      "ğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì„±ê³µ: ì˜ˆ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pymysql\n",
    "import re\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# DB ì ‘ì† ì •ë³´\n",
    "DB_CONFIG = {\n",
    "    'host': 'localhost',\n",
    "    'user': 'admin',\n",
    "    'password': '1qazZAQ!',\n",
    "    'db': 'final',\n",
    "    'charset': 'utf8mb4'\n",
    "}\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì•„ë˜ í•­ëª©ì„ í•œê¸€ë¡œ í•œ ì¤„ì”© ì¶”ì¶œí•´ì¤˜.\n",
    "\n",
    "title: ë¬¸ì„œì˜ ì œëª©ì„ í•œ ì¤„ë¡œ,\n",
    "summary: ì „ì²´ ë‚´ìš©ì„ 1000ì ì´ë‚´ë¡œ ì¤„ê±°ë¦¬ì²˜ëŸ¼ ìš”ì•½í•´ì¤˜. (ë„ì–´ì“°ê¸° í¬í•¨ 1000ì ì´í•˜, ë„ˆë¬´ ì§§ê²Œ ì“°ì§€ ë§ê³  ìµœëŒ€í•œ ìì„¸íˆ)\n",
    "keywords: ë¬¸ì„œì˜ í•µì‹¬ ë‹¨ì–´ë¥¼ 50ê°œ ì •ë„, ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ì„œ í•œ ì¤„ë¡œ ë‚˜ì—´í•´ì¤˜.\n",
    "\n",
    "ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•´ì¤˜.\n",
    "title: ...\n",
    "summary: ...\n",
    "keywords: ...\n",
    "\n",
    "ë¬¸ì„œ ë‚´ìš©:\n",
    "\"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "CUSTOM_PROMPT = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "# íŒŒì¼ í™•ì¥ì ì¶”ì¶œ í•¨ìˆ˜\n",
    "def get_doc_type(file_name):\n",
    "    \"\"\"íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì¶”ì¶œ (.í¬í•¨)\"\"\"\n",
    "    ext = os.path.splitext(file_name)[1].lower()\n",
    "    return ext if ext else \".unknown\"\n",
    "\n",
    "# íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° í•¨ìˆ˜\n",
    "def load_document(file_path: str):\n",
    "    \"\"\"ì•ˆì •ì ì¸ ë¬¸ì„œ ë¡œë”\"\"\"\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    try:\n",
    "        if ext == \".docx\":\n",
    "            from langchain_community.document_loaders import Docx2txtLoader\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "            \n",
    "        elif ext == \".pdf\":\n",
    "            from langchain_community.document_loaders import PyPDFLoader\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            \n",
    "        elif ext == \".csv\":\n",
    "            from langchain_community.document_loaders import CSVLoader\n",
    "            loader = CSVLoader(file_path, encoding=\"utf-8\")\n",
    "            \n",
    "        elif ext == \".txt\":\n",
    "            from langchain_community.document_loaders import TextLoader\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            \n",
    "        elif ext in (\".html\", \".htm\"):\n",
    "            from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "            loader = UnstructuredHTMLLoader(file_path)\n",
    "            \n",
    "        else:\n",
    "            from langchain_community.document_loaders import TextLoader\n",
    "            loader = TextLoader(file_path, encoding=\"utf-8\")\n",
    "            \n",
    "        docs = loader.load()\n",
    "        \n",
    "        # ë¹ˆ ë¬¸ì„œ í•„í„°ë§\n",
    "        docs = [doc for doc in docs if doc.page_content.strip()]\n",
    "        \n",
    "        return docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ {file_path} ë¡œë” ì—ëŸ¬: {e}\")\n",
    "        return []\n",
    "\n",
    "# LLM ìš”ì•½ í•¨ìˆ˜ (LangChain v0.2+ í˜¸í™˜)\n",
    "def summarize_with_llm(docs):\n",
    "    \"\"\"ë¬¸ì„œ ìš”ì•½ ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        llm = Ollama(model=\"exaone3.5:2.4b\")\n",
    "        \n",
    "        # ë¬¸ì„œê°€ ë§ìœ¼ë©´ ë§¨ ì• 5ê°œë§Œ ì‚¬ìš©\n",
    "        if len(docs) > 5:\n",
    "            docs = docs[:5]\n",
    "            print(f\"ğŸ“„ ë¬¸ì„œ ì²­í¬ê°€ ë§ì•„ ìƒìœ„ 5ê°œë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ë¬¸ì„œ ë‚´ìš© ê²°í•©\n",
    "        combined_content = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "        \n",
    "        # Prompt + LLM ì¡°í•© (Runnable Sequence)\n",
    "        chain = CUSTOM_PROMPT | llm\n",
    "        \n",
    "        # ì‹¤í–‰\n",
    "        result = chain.invoke({\"text\": combined_content})\n",
    "        \n",
    "        return result  # ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LLM ìš”ì•½ ì—ëŸ¬: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# LLM ê²°ê³¼ íŒŒì‹± í•¨ìˆ˜\n",
    "def parse_llm_output(output_text):\n",
    "    \"\"\"LLM ì¶œë ¥ íŒŒì‹±\"\"\"\n",
    "    try:\n",
    "        title_match = re.search(r\"title\\s*:\\s*(.+?)(?=\\n|summary:|$)\", output_text, re.DOTALL)\n",
    "        summary_match = re.search(r\"summary\\s*:\\s*(.+?)(?=\\n|keywords:|$)\", output_text, re.DOTALL)\n",
    "        keywords_match = re.search(r\"keywords\\s*:\\s*(.+?)(?=\\n|$)\", output_text, re.DOTALL)\n",
    "        \n",
    "        result = {\n",
    "            \"title\": title_match.group(1).strip() if title_match else \"ì œëª© ì—†ìŒ\",\n",
    "            \"summary\": summary_match.group(1).strip() if summary_match else \"ìš”ì•½ ì—†ìŒ\",\n",
    "            \"keywords\": keywords_match.group(1).strip() if keywords_match else \"í‚¤ì›Œë“œ ì—†ìŒ\",\n",
    "        }\n",
    "        \n",
    "        if len(result[\"summary\"]) > 1000:\n",
    "            result[\"summary\"] = result[\"summary\"][:997] + \"...\"\n",
    "            \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ íŒŒì‹± ì—ëŸ¬: {e}\")\n",
    "        return {\n",
    "            \"title\": \"íŒŒì‹± ì‹¤íŒ¨\",\n",
    "            \"summary\": \"ìš”ì•½ ìƒì„± ì‹¤íŒ¨\",\n",
    "            \"keywords\": \"í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨\"\n",
    "        }\n",
    "\n",
    "# DBì— INSERT í•¨ìˆ˜\n",
    "def insert_into_db(title, summary, keywords, file_location, file_name, doc_type):\n",
    "    \"\"\"DB ì €ì¥\"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = pymysql.connect(**DB_CONFIG)\n",
    "        with conn.cursor() as cursor:\n",
    "            sql = \"\"\"\n",
    "            INSERT INTO documents\n",
    "            (title, summary, keywords, file_location, file_name, doc_type, created_at)\n",
    "            VALUES (%s, %s, %s, %s, %s, %s, NOW())\n",
    "            \"\"\"\n",
    "            cursor.execute(sql, (title, summary, keywords, file_location, file_name, doc_type))\n",
    "        conn.commit()\n",
    "        print(f\"âœ… {file_name} DB ì €ì¥ ì™„ë£Œ!\")\n",
    "        print(f\"   íŒŒì¼íƒ€ì…: {doc_type}\")\n",
    "        print(f\"   ì œëª©: {title[:30]}...\")\n",
    "        print(f\"   ìš”ì•½: {summary[:50]}...\")\n",
    "        print(\"=============================================================\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {file_name} DB ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "        \n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "# ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def process_single_file(file_path, file_name):\n",
    "    \"\"\"ê°œë³„ íŒŒì¼ ì²˜ë¦¬\"\"\"\n",
    "    print(f\"ğŸ“„ ì²˜ë¦¬ ì¤‘: {file_path}\")\n",
    "    \n",
    "    doc_type = get_doc_type(file_name)\n",
    "    print(f\"ğŸ“‹ íŒŒì¼ íƒ€ì…: {doc_type}\")\n",
    "    \n",
    "    docs = load_document(file_path)\n",
    "    if not docs:\n",
    "        print(f\"âŒ {file_name} ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ë¬¸ì„œ\")\n",
    "        return False\n",
    "        \n",
    "    print(f\"âœ… {file_name} ë¡œë“œ ì™„ë£Œ. ì²­í¬ ìˆ˜: {len(docs)}\")\n",
    "    \n",
    "    llm_output = summarize_with_llm(docs)\n",
    "    if not llm_output:\n",
    "        print(f\"âŒ {file_name} LLM ìš”ì•½ ì‹¤íŒ¨\")\n",
    "        return False\n",
    "        \n",
    "    parsed = parse_llm_output(llm_output)\n",
    "    \n",
    "    insert_into_db(\n",
    "        title=parsed[\"title\"],\n",
    "        summary=parsed[\"summary\"],\n",
    "        keywords=parsed[\"keywords\"],\n",
    "        file_location=file_path,\n",
    "        file_name=file_name,\n",
    "        doc_type=doc_type\n",
    "    )\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ë©”ì¸ ì‹¤í–‰ (ë‹¨ì¼ íŒŒì¼ ì§€ì •)\n",
    "if __name__ == \"__main__\":\n",
    "    # ------------------- ì—¬ê¸°ë§Œ ìˆ˜ì • -------------------\n",
    "    # ì²˜ë¦¬í•  ë‹¨ì¼ íŒŒì¼ ê²½ë¡œ (ì ˆëŒ€/ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ ê°€ëŠ¥)\n",
    "    SINGLE_FILE_PATH = \"fileList/êµ­ë¯¼ì˜êµ°ëŒ€.docx\"\n",
    "    \n",
    "    if not os.path.exists(SINGLE_FILE_PATH):\n",
    "        print(f\"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {SINGLE_FILE_PATH}\")\n",
    "        exit(1)\n",
    "    \n",
    "    file_name = os.path.basename(SINGLE_FILE_PATH)\n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸\n",
    "    try:\n",
    "        import docx2txt\n",
    "        print(\"âœ… docx2txt íŒ¨í‚¤ì§€ í™•ì¸ë¨\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ docx2txt íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install docx2txt\")\n",
    "        exit(1)\n",
    "        \n",
    "    try:\n",
    "        import pypdf\n",
    "        print(\"âœ… pypdf íŒ¨í‚¤ì§€ í™•ì¸ë¨\")\n",
    "    except ImportError:\n",
    "        print(\"âŒ pypdf íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤: pip install pypdf\")\n",
    "        exit(1)\n",
    "    \n",
    "    # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬\n",
    "    print(f\"ğŸ” ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {SINGLE_FILE_PATH}\")\n",
    "    success = process_single_file(SINGLE_FILE_PATH, file_name)\n",
    "    print(f\"\\nğŸ‰ ì²˜ë¦¬ ì™„ë£Œ! ì„±ê³µ: {'ì˜ˆ' if success else 'ì•„ë‹ˆì˜¤'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bad4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "file_upload",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
